# Multimodal Emotion Analysis ğŸ­ğŸ§

A real-time and offline **multimodal emotion recognition system** that analyzes human emotions by fusing **speech (audio)** and **facial expressions (video)**.  
This project demonstrates the complete AI pipeline â€” from data preprocessing and model inference to multimodal fusion and real-time deployment.

---

## ğŸ” Overview

Human emotions are complex and cannot be reliably inferred from a single modality.  
This project leverages **audio and visual cues together** to improve robustness and accuracy in emotion recognition.

The system:
- Extracts emotions from **speech signals**
- Detects emotions from **facial expressions**
- Combines predictions using a **confidence-weighted late fusion strategy**

---

## âœ¨ Key Features

- ğŸ™ï¸ **Speech Emotion Recognition**
  - Uses a pretrained **Wav2Vec2** model fine-tuned for emotion classification
  - Supports offline audio inference

- ğŸ“¹ **Facial Emotion Recognition**
  - Face detection using OpenCV
  - Modular design to plug in pretrained CNN-based FER models

- ğŸ”— **Multimodal Late Fusion**
  - Confidence-weighted fusion of audio and video predictions
  - Handles noisy or missing modalities gracefully

- âš¡ **Real-Time Inference**
  - Live microphone input
  - Webcam-based facial analysis
  - Temporal-friendly architecture for future smoothing extensions

---

## ğŸ§  Architecture

