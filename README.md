# Multimodal Emotion Analysis ğŸ­ğŸ§

A real-time and offline **multimodal emotion recognition system** that analyzes human emotions by fusing **speech (audio)** and **facial expressions (video)**.  
This project demonstrates the complete AI pipeline â€” from data preprocessing and model inference to multimodal fusion and real-time deployment.

---

## ğŸ” Overview

Human emotions are complex and cannot be reliably inferred from a single modality.  
This project leverages **audio and visual cues together** to improve robustness and accuracy in emotion recognition.

The system:
- Extracts emotions from **speech signals**
- Detects emotions from **facial expressions**
- Combines predictions using a **confidence-weighted late fusion strategy**

---

## âœ¨ Key Features

- ğŸ™ï¸ **Speech Emotion Recognition**
  - Uses a pretrained **Wav2Vec2** model fine-tuned for emotion classification
  - Supports offline audio inference

- ğŸ“¹ **Facial Emotion Recognition**
  - Face detection using OpenCV
  - Modular design to plug in pretrained CNN-based FER models

- ğŸ”— **Multimodal Late Fusion**
  - Confidence-weighted fusion of audio and video predictions
  - Handles noisy or missing modalities gracefully

- âš¡ **Real-Time Inference**
  - Live microphone input
  - Webcam-based facial analysis
  - Temporal-friendly architecture for future smoothing extensions

---

## ğŸ› ï¸ Tech Stack

- **Programming Language:** Python  
- **Deep Learning:** PyTorch, Hugging Face Transformers  
- **Audio Processing:** Librosa, Torchaudio  
- **Computer Vision:** OpenCV  
- **Utilities:** NumPy, SciPy  
- **Hardware Support:** CPU / CUDA (GPU if available)

---

## ğŸ“Š Models & Datasets

### Audio Emotion Recognition
- **Model:** `superb/wav2vec2-base-superb-er`
- **Datasets Used:**
  - TESS (Toronto Emotional Speech Set)
  - CREMA-D

### Facial Emotion Recognition
- Face detection using Haar Cascades
- Emotion inference module designed to be easily replaceable with CNN-based FER models

---

## ğŸ“ Project Structure

multimodal-emotion-analysis/
â”‚â”€â”€ app.py
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ README.md
â”‚
â”œâ”€â”€ models/
â”‚ â”œâ”€â”€ audio_emotion.py
â”‚ â””â”€â”€ face_emotion.py
â”‚
â”œâ”€â”€ utils/
â”‚ â”œâ”€â”€ audio_utils.py
â”‚ â”œâ”€â”€ video_utils.py
â”‚ â””â”€â”€ fusion.py
â”‚
â”œâ”€â”€ data
â”‚ â””â”€â”€ sample_audio.wav
â”‚
â””â”€â”€ notebooks/
â””â”€â”€ experiments.ipynb



